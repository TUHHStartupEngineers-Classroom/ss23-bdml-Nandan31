[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "",
    "text": "Use your learning from descriptive features and plot_ggpairs() to further investigate the features. Run the functions above according to the features needed. Answer the following questions. Most of the time, you will only need the images from diagonal."
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#compensation-features-hourlyrate-monthlyincome-stockoptionlevel",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#compensation-features-hourlyrate-monthlyincome-stockoptionlevel",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.1 Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel",
    "text": "20.1 Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n  plot_ggpairs(Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n\nQ1.What can you deduce about the interaction between Monthly Income and Attrition?\n\n(a)Those that are leaving the company have a higher Monthly Income\n(b)That those are staying have a lower Monthly Income\n(c)Those that are leaving have a lower Monthly Income\n(d)It’s difficult to deduce anything based on the visualization\n\n\n\nQ2.What can you deduce about the interaction between Percent Salary Hike and Attrition?\n\n(a)Those that are leaving the company have a higher Percent Salary Hike\n(b)Those that are staying have a lower Percent Salary Hike\n(c)Those that are leaving have lower Percent Salary Hike\n(d)It’s difficult to deduce anything based on the visualization\n\n\n\nQ3.What can you deduce about the interaction between Stock Option Level and Attrition?\n\n(a)Those that are leaving the company have a higher stock option level\n(b)Those that are staying have a higher stock option level\n(c)It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#survey-results-satisfaction-level-work-life-balance",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#survey-results-satisfaction-level-work-life-balance",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.2 Survey Results: Satisfaction level, Work Life Balance",
    "text": "20.2 Survey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\nQ4.What can you deduce about the interaction between Environment Satisfaction and Attrition?\n\n(a)A higher proportion of those leaving have a low environment satisfaction level\n(b)A higher proportion of those leaving have a high environment satisfaction level\n(c)It’s difficult to deduce anything based on the visualization\n\n\n\nQ5.What can you deduce about the interaction between Work Life Balance and Attrition\n\n(a)Those that are leaving have higher density of 2’s and 3’s\n(b)Those that are staying have a higher density of 2’s and 3’s\n(c)Those that are staying have a lower density of 2’s and 3’s * (d)It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#performance-data-job-involvement-performance-rating",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#performance-data-job-involvement-performance-rating",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.3 Performance Data: Job Involvement, Performance Rating",
    "text": "20.3 Performance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\nQ6.What Can you deduce about the interaction between Job Involvement and Attrition?\n\n(a)Those that are leaving have a lower density of 3’s and 4’s\n(b)Those that are leaving have a lower density of 1’s and 2’s\n(c)Those that are staying have a lower density of 2’s and 3’s\n(d)It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#work-life-features-1",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#work-life-features-1",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.4 Work-Life Features",
    "text": "20.4 Work-Life Features\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\nQ7.What can you deduce about the interaction between Over Time and Attrition?\n\n(a)The proportion of those leaving that are working Over Time are high compared to those that are not leaving\n(b)The proportion of those staying that are working Over Time are high compared to those that are not staying"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#training-and-education",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#training-and-education",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.5 Training and Education",
    "text": "20.5 Training and Education\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\nQ8.What can you deduce about the interaction between Training Times Last Year and Attrition\n (a)People that leave tend to have more annual trainings  (b)People that leave tend to have less annual trainings * (c)It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/03 Automated Machine Learning with H20-1.html#time-based-features-years-at-company-years-in-current-role",
    "href": "content/01_journal/03 Automated Machine Learning with H20-1.html#time-based-features-years-at-company-years-in-current-role",
    "title": "03 Automated Machine Learning with H20-1",
    "section": "20.6 Time-Based Features: Years at company, years in current role",
    "text": "20.6 Time-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"years\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n\nQ9.What can you deduce about the interaction between Years At Company and Attrition\n\n(a)People that leave tend to have more working years at the company\n(b)People that leave tend to have less working years at the company\n(c)It’s difficult to deduce anything based on the visualization\n\n\n\nQ10.What can you deduce about the interaction between Years Since Last Promotion and Attrition?\n\n(a)Those that are leaving have more years since last promotion than those that are staying\n(b)Those that are leaving have fewer years since last promotion than those that are staying\n(c)It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02 Supervised ML.html",
    "href": "content/01_journal/02 Supervised ML.html",
    "title": "02 Supervised ML",
    "section": "",
    "text": "1 Challenge Summary\nIn this session we did not use the recipes packages to prepare our data. This is going to be your challenge. For further information take a look at the last session or just use google. Prepare the data for the models with the steps provided below. Remember, you don’t need to set the flags by yourself (see all_nominal()). # Load libraries\n\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Modeling\nlibrary(parsnip)\nlibrary(tidymodels)\n\n#> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n#> ✔ broom        1.0.4     ✔ rsample      1.1.1\n#> ✔ dials        1.2.0     ✔ tune         1.1.1\n#> ✔ infer        1.0.4     ✔ workflows    1.1.3\n#> ✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n#> ✔ recipes      1.0.6     ✔ yardstick    1.2.0\n#> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ scales::discard() masks purrr::discard()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ recipes::fixed()  masks stringr::fixed()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ yardstick::spec() masks readr::spec()\n#> ✖ recipes::step()   masks stats::step()\n#> • Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(rstanarm)\n\n#> Loading required package: Rcpp\n#> \n#> Attaching package: 'Rcpp'\n#> \n#> The following object is masked from 'package:rsample':\n#> \n#>     populate\n#> \n#> This is rstanarm version 2.21.4\n#> - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#> - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#> - For execution on a local, multicore CPU with excess RAM we recommend calling\n#>   options(mc.cores = parallel::detectCores())\n\nlibrary(broom.mixed)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\n#> Loading required package: rpart\n#> \n#> Attaching package: 'rpart'\n#> \n#> The following object is masked from 'package:dials':\n#> \n#>     prune\n\n\n\n\n2 Modeling\n\nbike_data_tbl <- readRDS(\"C:/Users/srajs/OneDrive/Desktop/Business/Business Decisions with Machine Learning/bike_orderlines.rds\")\nbike_data_tbl\n\n\n\n  \n\n\nmodel_sales_tbl <- bike_data_tbl %>%\n  select(total_price, model, category_2, frame_material) %>%\n  \n  group_by(model, category_2, frame_material) %>%\n  summarise(total_sales = sum(total_price)) %>%\n  ungroup() %>%\n  \n  arrange(desc(total_sales))\n\n#> `summarise()` has grouped output by 'model', 'category_2'. You can override\n#> using the `.groups` argument.\n\nmodel_sales_tbl %>%\n  mutate(category_2 = as_factor(category_2) %>% \n           fct_reorder(total_sales, .fun = max) %>% \n           fct_rev()) %>%\n  \n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(title = \"Total Sales for Each Model\",x = \"Frame Material\", y = \"Revenue\")\n\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n#> Warning in max(data$density): no non-missing arguments to max; returning -Inf\n\n\n#> Warning: Computation failed in `stat_ydensity()`\n#> Caused by error in `$<-.data.frame`:\n#> ! replacement has 1 row, data has 0\n\n\n\n\n\n\n\n\nbike_features_tbl <- readRDS(\"C:/Users/srajs/OneDrive/Desktop/Business/Business Decisions with Machine Learning/bike_features_tbl.rds\")\nbike_features_tbl <- bike_features_tbl %>% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %>% \n  mutate(`shimano dura-ace`        = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano dura-ace \") %>% as.numeric(),\n    `shimano ultegra`         = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano ultegra \") %>% as.numeric(),\n    `shimano 105`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano 105 \") %>% as.numeric(),\n    `shimano tiagra`          = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano tiagra \") %>% as.numeric(),\n    `Shimano sora`            = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano sora\") %>% as.numeric(),\n    `shimano deore`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano deore(?! xt)\") %>% as.numeric(),\n    `shimano slx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano slx\") %>% as.numeric(),\n    `shimano grx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano grx\") %>% as.numeric(),\n    `Shimano xt`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano deore xt |shimano xt \") %>% as.numeric(),\n    `Shimano xtr`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano xtr\") %>% as.numeric(),\n    `Shimano saint`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano saint\") %>% as.numeric(),\n    `SRAM red`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram red\") %>% as.numeric(),\n    `SRAM force`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram force\") %>% as.numeric(),\n    `SRAM rival`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram rival\") %>% as.numeric(),\n    `SRAM apex`               = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram apex\") %>% as.numeric(),\n    `SRAM xx1`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram xx1\") %>% as.numeric(),\n    `SRAM x01`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram x01|sram xo1\") %>% as.numeric(),\n    `SRAM gx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram gx\") %>% as.numeric(),\n    `SRAM nx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram nx\") %>% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram sx\") %>% as.numeric(),\n    `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram sx\") %>% as.numeric(),\n    `Campagnolo potenza`      = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"campagnolo potenza\") %>% as.numeric(),\n    `Campagnolo super record` = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"campagnolo super record\") %>% as.numeric(),\n    `shimano nexus`           = `Shift Lever`     %>% str_to_lower() %>% str_detect(\"shimano nexus\") %>% as.numeric(),\n    `shimano alfine`          = `Shift Lever`     %>% str_to_lower() %>% str_detect(\"shimano alfine\") %>% as.numeric()\n  ) %>%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %>% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \n\n\nbike_features_tbl <- bike_features_tbl %>% \n  mutate(id = row_number()) %>% \n  mutate(frame_material = factor(frame_material)) %>%\n  select(id, everything()) \n\n\nbike_features_tbl %>% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj <- rsample::initial_split(bike_features_tbl, prop   = 0.80, \n                                    strata = \"category_2\")\nsplit_obj %>% training() %>% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %>% testing() %>% distinct(category_2)\n\n\n\n  \n\n\ntrain_tbl <- training(split_obj)\ntest_tbl  <- testing(split_obj)\n\n\ntrain_data <- train_tbl %>% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_data  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl), \" |-\", \"_\"))\n\n\n\n3 Recipe\n\nbike_recipe <- recipe(frame_material ~ ., data = train_data) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) \nd <- summary(bike_recipe)\nlr_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\nlr_mod\n\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n\nbike_workflow <- workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(bike_recipe)\nbike_workflow\n\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_dummy()\n#> • step_zv()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n\nbike_fit <- bike_workflow %>% \n  fit(data = train_data)\n\n#> Warning: glm.fit: algorithm did not converge\n\n\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nbike_fit\n\n#> ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_dummy()\n#> • step_zv()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> \n#> Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#>                (Intercept)                          id  \n#>                 -8.034e+15                   9.865e+13  \n#>                     weight                       price  \n#>                 -5.391e+14                   4.195e+11  \n#>           shimano_dura_ace             shimano_ultegra  \n#>                  4.607e+15                   5.223e+15  \n#>                shimano_105              shimano_tiagra  \n#>                  4.165e+15                   1.624e+15  \n#>              shimano_deore                 shimano_slx  \n#>                 -2.068e+15                  -2.704e+15  \n#>                shimano_grx                  Shimano_xt  \n#>                  8.945e+14                  -2.235e+15  \n#>              Shimano_saint                    SRAM_red  \n#>                  7.651e+15                   2.189e+15  \n#>                 SRAM_force                  SRAM_rival  \n#>                  5.047e+15                   1.823e+15  \n#>                  SRAM_apex                    SRAM_xx1  \n#>                  2.171e+14                  -2.914e+15  \n#>                   SRAM_x01                     SRAM_gx  \n#>                 -5.055e+14                  -7.888e+14  \n#>                    SRAM_nx                     SRAM_sx  \n#>                  3.084e+15                  -3.721e+15  \n#>         Campagnolo_potenza     Campagnolo_super_record  \n#>                  4.116e+15                  -7.795e+14  \n#>              shimano_nexus              shimano_alfine  \n#>                 -1.229e+15                  -1.403e+15  \n#>          category_1_Gravel    category_1_Hybrid...City  \n#>                  1.214e+15                  -1.015e+16  \n#>        category_1_Mountain             category_1_Road  \n#>                 -1.163e+15                   5.451e+15  \n#>        category_2_All.Road             category_2_City  \n#>                 -3.549e+14                  -4.653e+15  \n#>   category_2_Cross.Country       category_2_Cyclocross  \n#>                  3.413e+15                   5.973e+14  \n#>       category_2_Dirt.Jump         category_2_Downhill  \n#>                 -6.743e+15                  -2.246e+15  \n#>          category_2_E.City        category_2_E.Fitness  \n#>                  2.896e+15                  -8.811e+15  \n#>        category_2_E.Gravel       category_2_E.Mountain  \n#>                  8.503e+15                   5.331e+15  \n#>      category_2_E.Trekking        category_2_Endurance  \n#>                         NA                  -1.016e+15  \n#>          category_2_Enduro        category_2_Fat.Bikes  \n#>                 -3.238e+14                   8.274e+15  \n#>            category_2_Race          category_2_Touring  \n#>                  3.653e+15                          NA  \n#> \n#> ...\n#> and 34 more lines.\n\nbike_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n#> Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\n#> ℹ Please use `extract_fit_parsnip()` instead.\n\n\n\n\n  \n\n\nbike_predict <- predict(bike_fit, test_data, type=\"prob\") %>% \n  bind_cols(test_data %>% select(frame_material, category_2)) \n\n#> Warning: There are new levels in a factor: E-Road\n\n\n#> Warning: There are new levels in a factor: Endurace:ON\n\n\n#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\n#> prediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\nbike_predict %>% \n  roc_curve(truth = frame_material, .pred_aluminium) %>% \n  autoplot()\n\n\n\n\n\n\n\nbike_predict %>% \n  roc_curve(truth = frame_material, .pred_carbon) %>% \n  autoplot()\n\n\n\n\n\n\n\nbike_predict\n\n\n\n  \n\n\nroc_al <- bike_predict %>% \n  roc_auc(truth = frame_material, .pred_aluminium)\nroc_al\n\n\n\n  \n\n\nroc_car <- bike_predict %>% \n  roc_auc(truth = frame_material, .pred_carbon)\nroc_car\n\n\n\n  \n\n\n\n\n\n4 Evaluation\n\nmodel_01_linear_lm_simple <- linear_reg(mode = \"regression\") %>%\n  set_engine(\"lm\") %>%\n  fit(price ~ category_2 + frame_material, data = train_data)\nmodel_01_linear_lm_simple\n\n#> parsnip model object\n#> \n#> \n#> Call:\n#> stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#> \n#> Coefficients:\n#>              (Intercept)        category_2All-Road            category_2City  \n#>                   1971.0                    -353.2                   -1068.7  \n#>  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#>                    573.5                    -304.0                    -705.3  \n#>       category_2Downhill          category_2E-City       category_2E-Fitness  \n#>                    674.0                    1125.2                    1071.3  \n#>       category_2E-Gravel      category_2E-Mountain      category_2E-Trekking  \n#>                   1370.0                    1207.6                    1612.0  \n#>      category_2Endurance          category_2Enduro       category_2Fat Bikes  \n#>                   -205.2                     665.5                    -970.0  \n#>           category_2Race         category_2Touring           category_2Trail  \n#>                    913.5                    -702.0                    -109.0  \n#> category_2Triathlon Bike      frame_materialcarbon  \n#>                    815.4                    1628.0\n\ntest_data <- test_data %>% filter(category_2 != \"Fat Bikes\")%>% filter(category_2 != \"E-Road\")\nyards <- model_01_linear_lm_simple %>%\n  predict(new_data = test_data) %>%\n  bind_cols(test_data %>% select(price)) %>%\n  yardstick::metrics(truth = price, estimate = .pred)\nyards\n\n\n\n  \n\n\ng1 <- bike_features_tbl %>% \n  mutate(category_2 = as.factor(category_2) %>% \n           fct_reorder(price)) %>% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs( title = \"Price for Each Model\", y = \"\", x = \"Category 2\")\ng1\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n\n\n\n\n\n\nnew_trail <- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Trail\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \nnew_trail\n\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_trail)\n\n\n\n  \n\n\nmodels_tbl <- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple\n  )\n)\nmodels_tbl\n\n\n\n  \n\n\npredictions_new_trail <- models_tbl %>%\n  mutate(predictions = map(model, predict, new_data = new_trail)) %>%\n  unnest(predictions) %>%\n  mutate(category_2 = \"Trail\") %>%\n  left_join(new_trail, by = \"category_2\")\npredictions_new_trail\n\n\n\n  \n\n\ng2 <- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_trail) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 5,\n                           data = predictions_new_trail)\ng2\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped."
  },
  {
    "objectID": "content/01_journal/05 Performance Measures.html",
    "href": "content/01_journal/05 Performance Measures.html",
    "title": "05 Performance Measures",
    "section": "",
    "text": "1 Task\nApply all the steps you have learned in this session on the dataset from challenge of the last session (Product Backorders):\n\nLeaderboard visualization\nTune a model with grid search\nVisualize the trade of between the precision and the recall and the optimal threshold\nROC Plot\nPrecision vs Recall Plot\nGain Plot\nLift Plot\nDashboard with cowplot\n\n\n#load libraries ::: {.cell hash=‘05-Performance-Measures_cache/html/unnamed-chunk-1_a0a68bb92fe9cd5dc520d6d6b5fc22a4’}\nlibrary(tidymodels)\n\n#> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n#> ✔ broom        1.0.4     ✔ recipes      1.0.6\n#> ✔ dials        1.2.0     ✔ rsample      1.1.1\n#> ✔ dplyr        1.1.2     ✔ tibble       3.2.1\n#> ✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n#> ✔ infer        1.0.4     ✔ tune         1.1.1\n#> ✔ modeldata    1.1.0     ✔ workflows    1.1.3\n#> ✔ parsnip      1.1.0     ✔ workflowsets 1.0.1\n#> ✔ purrr        1.0.1     ✔ yardstick    1.2.0\n\n\n#> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ purrr::discard() masks scales::discard()\n#> ✖ dplyr::filter()  masks stats::filter()\n#> ✖ dplyr::lag()     masks stats::lag()\n#> ✖ recipes::step()  masks stats::step()\n#> • Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(magrittr)\n\n#> \n#> Attaching package: 'magrittr'\n\n\n#> The following object is masked from 'package:tidyr':\n#> \n#>     extract\n\n\n#> The following object is masked from 'package:purrr':\n#> \n#>     set_names\n\nlibrary(dplyr)\nlibrary(sjmisc)\n\n#> Install package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\n\n#> \n#> Attaching package: 'sjmisc'\n\n\n#> The following object is masked from 'package:workflows':\n#> \n#>     add_variables\n\n\n#> The following object is masked from 'package:tidyr':\n#> \n#>     replace_na\n\n\n#> The following object is masked from 'package:tibble':\n#> \n#>     add_case\n\n\n#> The following object is masked from 'package:purrr':\n#> \n#>     is_empty\n\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\n\n#> \n#> Attaching package: 'sjlabelled'\n\n\n#> The following objects are masked from 'package:haven':\n#> \n#>     as_factor, read_sas, read_spss, read_stata, write_sas, zap_labels\n\n\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     as_label\n\n\n#> The following object is masked from 'package:dplyr':\n#> \n#>     as_label\n\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\n\n#> Loading required package: Rcpp\n\n\n#> \n#> Attaching package: 'Rcpp'\n\n\n#> The following object is masked from 'package:rsample':\n#> \n#>     populate\n\n\n#> This is rstanarm version 2.21.4\n\n\n#> - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n#> - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n#> - For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n#>   options(mc.cores = parallel::detectCores())\n\nlibrary(broom.mixed)\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n\n\n#> \n#> Attaching package: 'h2o'\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     cor, sd, var\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\nproduct_backorders_tbl <- read.csv(\"C:/Users/srajs/OneDrive/Desktop/Business/Business Decisions with Machine Learning/product_backorders.csv\")\nproduct_backorders_tbl %>% glimpse()\n\n#> Rows: 19,053\n#> Columns: 23\n#> $ sku               <int> 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n#> $ national_inv      <int> 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n#> $ lead_time         <int> 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n#> $ in_transit_qty    <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n#> $ forecast_3_month  <int> 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n#> $ forecast_6_month  <int> 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n#> $ forecast_9_month  <int> 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n#> $ sales_1_month     <int> 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n#> $ sales_3_month     <int> 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n#> $ sales_6_month     <int> 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n#> $ sales_9_month     <int> 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n#> $ min_bank          <int> 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n#> $ potential_issue   <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ pieces_past_due   <int> 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ perf_6_month_avg  <dbl> 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n#> $ perf_12_month_avg <dbl> 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n#> $ local_bo_qty      <int> 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n#> $ deck_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ oe_constraint     <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ ppap_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n#> $ stop_auto_buy     <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n#> $ rev_stop          <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ went_on_backorder <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n\n:::\n\n\n2 Assign training and test data\n\ndata_split <- initial_split(product_backorders_tbl, prop = 3/4)\n# Assign training and test data\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n# Specifying of the response and the predictor variables\nfactor_names <- c(\"went_on_backorder\")\nproduct_rec <- \n  recipe(went_on_backorder ~ ., data = train_data) %>%  \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_mutate_at(factor_names, fn = as.factor) %>%\n  prep()\n\n#> Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\n#> ℹ Please use `all_of()` or `any_of()` instead.\n#>   # Was:\n#>   data %>% select(factor_names)\n#> \n#>   # Now:\n#>   data %>% select(all_of(factor_names))\n#> \n#> See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n\nd <- summary(product_rec)\ntrain_tbl <- bake(product_rec, new_data = train_data)\ntest_tbl  <- bake(product_rec, new_data = test_data)\n\n\n\n3 Splitting the data for validation df\n\nh2o.init()\n\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     C:\\Users\\srajs\\AppData\\Local\\Temp\\Rtmpcjp8jz\\file137c6fe96f0a/h2o_srajs_started_from_r.out\n#>     C:\\Users\\srajs\\AppData\\Local\\Temp\\Rtmpcjp8jz\\file137c69542ea0/h2o_srajs_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting:  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 seconds 807 milliseconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    4 months and 11 days \n#>     H2O cluster name:           H2O_started_from_R_srajs_qzj612 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.47 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (4 months and 11 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n# Splitting the data for validation df\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.75), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Predictors\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n\n\n\n4 AutoML Model\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         3 seconds 802 milliseconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    4 months and 11 days \n#>     H2O cluster name:           H2O_started_from_R_srajs_qzj612 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.47 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (4 months and 11 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nh2o_models_automl <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5 \n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n#> 22:38:23.255: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 22:38:23.272: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(h2o_models_automl)\n\n#> [1] \"S4\"\n\nslotNames(h2o_models_automl)\n\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n\nh2o_models_automl@leaderboard \n\n#>                                                  model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_3_AutoML_1_20230619_223823 0.9526068 0.1674555\n#> 2    StackedEnsemble_AllModels_2_AutoML_1_20230619_223823 0.9501580 0.1706551\n#> 3 StackedEnsemble_BestOfFamily_4_AutoML_1_20230619_223823 0.9498832 0.1719480\n#> 4 StackedEnsemble_BestOfFamily_3_AutoML_1_20230619_223823 0.9498832 0.1719480\n#> 5    StackedEnsemble_AllModels_1_AutoML_1_20230619_223823 0.9493376 0.1715296\n#> 6                          GBM_4_AutoML_1_20230619_223823 0.9488975 0.1729766\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7558074            0.1311022 0.2235405 0.04997033\n#> 2 0.7451591            0.1550369 0.2252122 0.05072054\n#> 3 0.7409406            0.1326500 0.2250792 0.05066066\n#> 4 0.7409406            0.1326500 0.2250792 0.05066066\n#> 5 0.7434143            0.1518809 0.2253795 0.05079593\n#> 6 0.7386801            0.1246109 0.2267178 0.05140094\n#> \n#> [36 rows x 7 columns]\n\n#h2o_models_automl@leader\n\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 minutes 34 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    4 months and 11 days \n#>     H2O cluster name:           H2O_started_from_R_srajs_qzj612 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.31 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (4 months and 11 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nh2o.loadModel(\"C:/Users/srajs/OneDrive/Desktop/Business/Business Decisions with Machine Learning/h20models/StackedEnsemble_AllModels_3_AutoML_2_20230517_140427\")\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_3_AutoML_2_20230517_140427 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)             7/31\n#> 3           # GBM base models (used / total)             0/11\n#> 4       # XGBoost base models (used / total)             7/13\n#> 5           # DRF base models (used / total)              0/2\n#> 6  # DeepLearning base models (used / total)              0/4\n#> 7           # GLM base models (used / total)              0/1\n#> 8                      Metalearner algorithm              GLM\n#> 9         Metalearner fold assignment scheme           Random\n#> 10                        Metalearner nfolds                5\n#> 11                   Metalearner fold_column               NA\n#> 12        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.03540022\n#> RMSE:  0.1881495\n#> LogLoss:  0.1429984\n#> Mean Per-Class Error:  0.0252418\n#> AUC:  0.998568\n#> AUCPR:  0.9917009\n#> Gini:  0.997136\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error       Rate\n#> No     8809   36 0.004070   =36/8845\n#> Yes      55 1130 0.046414   =55/1185\n#> Totals 8864 1166 0.009073  =91/10030\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.303441    0.961293 167\n#> 2                       max f2  0.271269    0.962245 179\n#> 3                 max f0point5  0.348895    0.973251 152\n#> 4                 max accuracy  0.303441    0.990927 167\n#> 5                max precision  0.804001    1.000000   0\n#> 6                   max recall  0.119818    1.000000 260\n#> 7              max specificity  0.804001    1.000000   0\n#> 8             max absolute_mcc  0.303441    0.956195 167\n#> 9   max min_per_class_accuracy  0.241905    0.979747 190\n#> 10 max mean_per_class_accuracy  0.257387    0.981346 184\n#> 11                     max tns  0.804001 8845.000000   0\n#> 12                     max fns  0.804001 1183.000000   0\n#> 13                     max fps  0.014115 8845.000000 399\n#> 14                     max tps  0.119818 1185.000000 260\n#> 15                     max tnr  0.804001    1.000000   0\n#> 16                     max fnr  0.804001    0.998312   0\n#> 17                     max fpr  0.014115    1.000000 399\n#> 18                     max tpr  0.119818    1.000000 260\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.05707625\n#> RMSE:  0.2389064\n#> LogLoss:  0.1982575\n#> Mean Per-Class Error:  0.141324\n#> AUC:  0.9528366\n#> AUCPR:  0.7374882\n#> Gini:  0.9056733\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2974 154 0.049233  =154/3128\n#> Yes      95 312 0.233415    =95/407\n#> Totals 3069 466 0.070438  =249/3535\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.260029    0.714777 169\n#> 2                       max f2  0.152343    0.786899 229\n#> 3                 max f0point5  0.355099    0.727380 127\n#> 4                 max accuracy  0.355099    0.933239 127\n#> 5                max precision  0.766033    1.000000   0\n#> 6                   max recall  0.035210    1.000000 357\n#> 7              max specificity  0.766033    1.000000   0\n#> 8             max absolute_mcc  0.260029    0.676817 169\n#> 9   max min_per_class_accuracy  0.157301    0.891944 226\n#> 10 max mean_per_class_accuracy  0.152343    0.898120 229\n#> 11                     max tns  0.766033 3128.000000   0\n#> 12                     max fns  0.766033  405.000000   0\n#> 13                     max fps  0.014435 3128.000000 399\n#> 14                     max tps  0.035210  407.000000 357\n#> 15                     max tnr  0.766033    1.000000   0\n#> 16                     max fnr  0.766033    0.995086   0\n#> 17                     max fpr  0.014435    1.000000 399\n#> 18                     max tpr  0.035210    1.000000 357\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04799411\n#> RMSE:  0.2190756\n#> LogLoss:  0.1616083\n#> Mean Per-Class Error:  0.1547554\n#> AUC:  0.9566977\n#> AUCPR:  0.7641007\n#> Gini:  0.9133954\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     9104  383 0.040371   =383/9487\n#> Yes     341  926 0.269140   =341/1267\n#> Totals 9445 1309 0.067324  =724/10754\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.370591    0.718944 189\n#> 2                       max f2  0.148019    0.792296 271\n#> 3                 max f0point5  0.609833    0.746110 118\n#> 4                 max accuracy  0.466035    0.934164 160\n#> 5                max precision  0.984011    1.000000   0\n#> 6                   max recall  0.004063    1.000000 391\n#> 7              max specificity  0.984011    1.000000   0\n#> 8             max absolute_mcc  0.370591    0.680830 189\n#> 9   max min_per_class_accuracy  0.129432    0.895225 280\n#> 10 max mean_per_class_accuracy  0.104566    0.897349 294\n#> 11                     max tns  0.984011 9487.000000   0\n#> 12                     max fns  0.984011 1265.000000   0\n#> 13                     max fps  0.000994 9487.000000 399\n#> 14                     max tps  0.004063 1267.000000 391\n#> 15                     max tnr  0.984011    1.000000   0\n#> 16                     max fnr  0.984011    0.998421   0\n#> 17                     max fpr  0.000994    1.000000 399\n#> 18                     max tpr  0.004063    1.000000 391\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.931513  0.008698   0.934710   0.916017   0.935089   0.936721\n#> auc         0.956914  0.006003   0.950198   0.952987   0.961548   0.964613\n#> err         0.068487  0.008698   0.065290   0.083983   0.064911   0.063279\n#> err_count 147.400000 20.082330 136.000000 183.000000 138.000000 137.000000\n#> f0point5    0.705988  0.038221   0.727796   0.640871   0.711582   0.710974\n#>           cv_5_valid\n#> accuracy    0.935029\n#> auc         0.955226\n#> err         0.064970\n#> err_count 143.000000\n#> f0point5    0.738719\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.695108  0.052181   0.731405   0.607450   0.698885\n#> r2                  0.537947  0.018679   0.541385   0.507631   0.558318\n#> recall              0.760794  0.039897   0.713710   0.821705   0.767347\n#> residual_deviance 696.170170 50.273865 693.521400 743.264900 647.839970\n#> rmse                0.219064  0.006668   0.219320   0.226705   0.212212\n#> specificity         0.954351  0.014660   0.964578   0.928683   0.956938\n#>                   cv_4_valid cv_5_valid\n#> precision           0.699620   0.738182\n#> r2                  0.544979   0.537421\n#> recall              0.760331   0.740876\n#> residual_deviance 645.474000 750.750500\n#> rmse                0.212547   0.224538\n#> specificity         0.958918   0.962636\n\n extract_h2o_model_name_by_position <- function(h2o_leaderboard, n = 1, verbose = T) {\n   \n   model_name <- h2o_leaderboard %>%\n     as_tibble() %>%\n     slice(n) %>%\n     pull(model_id)\n   \n   if (verbose) message(model_name)\n   \n   return(model_name)\n   \n }\n\n\nstacked_ensemble_h2o <- h2o.loadModel(\"C:/Users/srajs/OneDrive/Desktop/Business/Business Decisions with Machine Learning/h20models/StackedEnsemble_AllModels_3_AutoML_2_20230517_140427\")\nstacked_ensemble_h2o\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_3_AutoML_2_20230517_140427 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)             7/31\n#> 3           # GBM base models (used / total)             0/11\n#> 4       # XGBoost base models (used / total)             7/13\n#> 5           # DRF base models (used / total)              0/2\n#> 6  # DeepLearning base models (used / total)              0/4\n#> 7           # GLM base models (used / total)              0/1\n#> 8                      Metalearner algorithm              GLM\n#> 9         Metalearner fold assignment scheme           Random\n#> 10                        Metalearner nfolds                5\n#> 11                   Metalearner fold_column               NA\n#> 12        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.03540022\n#> RMSE:  0.1881495\n#> LogLoss:  0.1429984\n#> Mean Per-Class Error:  0.0252418\n#> AUC:  0.998568\n#> AUCPR:  0.9917009\n#> Gini:  0.997136\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error       Rate\n#> No     8809   36 0.004070   =36/8845\n#> Yes      55 1130 0.046414   =55/1185\n#> Totals 8864 1166 0.009073  =91/10030\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.303441    0.961293 167\n#> 2                       max f2  0.271269    0.962245 179\n#> 3                 max f0point5  0.348895    0.973251 152\n#> 4                 max accuracy  0.303441    0.990927 167\n#> 5                max precision  0.804001    1.000000   0\n#> 6                   max recall  0.119818    1.000000 260\n#> 7              max specificity  0.804001    1.000000   0\n#> 8             max absolute_mcc  0.303441    0.956195 167\n#> 9   max min_per_class_accuracy  0.241905    0.979747 190\n#> 10 max mean_per_class_accuracy  0.257387    0.981346 184\n#> 11                     max tns  0.804001 8845.000000   0\n#> 12                     max fns  0.804001 1183.000000   0\n#> 13                     max fps  0.014115 8845.000000 399\n#> 14                     max tps  0.119818 1185.000000 260\n#> 15                     max tnr  0.804001    1.000000   0\n#> 16                     max fnr  0.804001    0.998312   0\n#> 17                     max fpr  0.014115    1.000000 399\n#> 18                     max tpr  0.119818    1.000000 260\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.05707625\n#> RMSE:  0.2389064\n#> LogLoss:  0.1982575\n#> Mean Per-Class Error:  0.141324\n#> AUC:  0.9528366\n#> AUCPR:  0.7374882\n#> Gini:  0.9056733\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2974 154 0.049233  =154/3128\n#> Yes      95 312 0.233415    =95/407\n#> Totals 3069 466 0.070438  =249/3535\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.260029    0.714777 169\n#> 2                       max f2  0.152343    0.786899 229\n#> 3                 max f0point5  0.355099    0.727380 127\n#> 4                 max accuracy  0.355099    0.933239 127\n#> 5                max precision  0.766033    1.000000   0\n#> 6                   max recall  0.035210    1.000000 357\n#> 7              max specificity  0.766033    1.000000   0\n#> 8             max absolute_mcc  0.260029    0.676817 169\n#> 9   max min_per_class_accuracy  0.157301    0.891944 226\n#> 10 max mean_per_class_accuracy  0.152343    0.898120 229\n#> 11                     max tns  0.766033 3128.000000   0\n#> 12                     max fns  0.766033  405.000000   0\n#> 13                     max fps  0.014435 3128.000000 399\n#> 14                     max tps  0.035210  407.000000 357\n#> 15                     max tnr  0.766033    1.000000   0\n#> 16                     max fnr  0.766033    0.995086   0\n#> 17                     max fpr  0.014435    1.000000 399\n#> 18                     max tpr  0.035210    1.000000 357\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04799411\n#> RMSE:  0.2190756\n#> LogLoss:  0.1616083\n#> Mean Per-Class Error:  0.1547554\n#> AUC:  0.9566977\n#> AUCPR:  0.7641007\n#> Gini:  0.9133954\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     9104  383 0.040371   =383/9487\n#> Yes     341  926 0.269140   =341/1267\n#> Totals 9445 1309 0.067324  =724/10754\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.370591    0.718944 189\n#> 2                       max f2  0.148019    0.792296 271\n#> 3                 max f0point5  0.609833    0.746110 118\n#> 4                 max accuracy  0.466035    0.934164 160\n#> 5                max precision  0.984011    1.000000   0\n#> 6                   max recall  0.004063    1.000000 391\n#> 7              max specificity  0.984011    1.000000   0\n#> 8             max absolute_mcc  0.370591    0.680830 189\n#> 9   max min_per_class_accuracy  0.129432    0.895225 280\n#> 10 max mean_per_class_accuracy  0.104566    0.897349 294\n#> 11                     max tns  0.984011 9487.000000   0\n#> 12                     max fns  0.984011 1265.000000   0\n#> 13                     max fps  0.000994 9487.000000 399\n#> 14                     max tps  0.004063 1267.000000 391\n#> 15                     max tnr  0.984011    1.000000   0\n#> 16                     max fnr  0.984011    0.998421   0\n#> 17                     max fpr  0.000994    1.000000 399\n#> 18                     max tpr  0.004063    1.000000 391\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.931513  0.008698   0.934710   0.916017   0.935089   0.936721\n#> auc         0.956914  0.006003   0.950198   0.952987   0.961548   0.964613\n#> err         0.068487  0.008698   0.065290   0.083983   0.064911   0.063279\n#> err_count 147.400000 20.082330 136.000000 183.000000 138.000000 137.000000\n#> f0point5    0.705988  0.038221   0.727796   0.640871   0.711582   0.710974\n#>           cv_5_valid\n#> accuracy    0.935029\n#> auc         0.955226\n#> err         0.064970\n#> err_count 143.000000\n#> f0point5    0.738719\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.695108  0.052181   0.731405   0.607450   0.698885\n#> r2                  0.537947  0.018679   0.541385   0.507631   0.558318\n#> recall              0.760794  0.039897   0.713710   0.821705   0.767347\n#> residual_deviance 696.170170 50.273865 693.521400 743.264900 647.839970\n#> rmse                0.219064  0.006668   0.219320   0.226705   0.212212\n#> specificity         0.954351  0.014660   0.964578   0.928683   0.956938\n#>                   cv_4_valid cv_5_valid\n#> precision           0.699620   0.738182\n#> r2                  0.544979   0.537421\n#> recall              0.760331   0.740876\n#> residual_deviance 645.474000 750.750500\n#> rmse                0.212547   0.224538\n#> specificity         0.958918   0.962636\n\npredictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#> [1] \"environment\"\n\npredictions_tbl <- predictions %>% as_tibble()\npredictions_tbl"
  }
]